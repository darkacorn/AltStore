{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darkacorn/AltStore/blob/main/Text_Generation_WebUI_(Simple).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmzc69IfrZ-g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "outputId": "3ea93ffc-4b38-4160-83fa-03ab34e66522"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title ðŸŽµ Run Silent Audio Player { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ðŸ‘‡ Press play on the audio player that appears below. This will keep Colab from disconnecting you for inactivity.\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UecGsZ88rsOF"
      },
      "outputs": [],
      "source": [
        "#@title ##**ðŸš€ Start TextGen WebUI**\n",
        "\n",
        "import sys, os, sys, base64, subprocess, json, shutil, requests, time, pathlib, multiprocessing\n",
        "from IPython.display import clear_output, display, HTML\n",
        "from IPython.utils import capture\n",
        "from google.colab import files, drive\n",
        "from PIL import Image\n",
        "\n",
        "#PARAMS\n",
        "#@markdown ðŸ‘ˆ Configure the settings below, then press this button to start the installation process. The links will appear at the bottom of this page after a few minutes.\n",
        "\n",
        "model = \"MythoMax L2 Kimiko v2 13B\" #@param [\"Airoboros L2 3.0 13B\", \"Airoboros L2 3.0 7B\", \"Airoboros L2 GPT4 2.0 13B\", \"Airoboros L2 GPT4 m2.0 13B\", \"Airoboros L2 GPT4 2.0 7B\", \"Airoboros L2 GPT4 m2.0 7B\", \"Airoboros M 3.0 7B\", \"Airoboros Mistral2.2 7B\", \"Airochronos L2 13B\", \"Amethyst Mistral 13B\", \"AppleSauce L2 13B\", \"Asclepius 13B\", \"Athena v3 13B\", \"Baichuan2 Chat 13B\", \"BerrySauce L2 13B\", \"Buddy v0.2 7B\", \"BlueMethod 13B\", \"CalliopeDS L2 13B\", \"Carl L2 13B\", \"Chronos-Beluga v2 13B\", \"Chronos-Hermes v2 13B\", \"CodeFuse 13B\", \"CodeLlama 13B\", \"CodeLlama 13B Instruct\", \"CodeLlama 13B Python\", \"CollectiveCognition v1.1 Mistral 7B\", \"Dans AdventurousWinds 7B\", \"Dolphin 2.0 Mistral 7B\", \"Dolphin L2 7B\", \"Emerhyst 13B\", \"Em German v01 13B\", \"Em German v01 7B\", \"Em German Mistral v01 7B\", \"Frankenstein's Monster 13B\", \"Guanaco L2 13B\", \"Guanaco L2 7B\", \"Guanaco Uncensored v2 3B\", \"Hermes-Kimiko 13B\", \"Huginn v1.2 13B\", \"Huginn v3 13B\", \"Huginn v4.5 13B\", \"JanniesBasedLigma L2 13B\", \"Kimiko v2 13B\", \"Kimiko 7B\", \"Koala 7B\", \"Kimiko-Mistral 7B\", \"Kuchiki 1.1 L2 7B\", \"Leo Hessianai 13B\", \"Leo Hessianai 7B\", \"Leo Hessianai Chat 13B\", \"Leo Hessianai Chat 7B\", \"Leo Hessianai Chat Bilingual 13B\", \"Leo Hessianai Chat Bilingual 7B\", \"Lince Zero 7B\", \"Llama 2 13B\", \"Llama 2 7B\", \"Llama 2 13B Chat\", \"Llama 2 7B Chat\", \"Llama 2 Arguments 7B\", \"Llama 2 Ensemble v6 13B\", \"Llama 2 Vietnamese 20k 7B\", \"LoKuS 13B\", \"LosslessMegaCoder L2 7B\", \"Luban 13B\", \"Luna AI L2 7B\", \"Magpie 13B\", \"MAmmoTH 13B\", \"MAmmoTH 7B\", \"MAmmoTH Coder 13B\", \"Marx 3B\", \"MegaMix A1 13B\", \"MegaMix S1 13B\", \"MegaMix T1 13B\", \"MetaMath V1.0 13B\", \"MetaMath V1.0 7B\", \"Metharme 13B\", \"Metharme 7B\", \"Mistral OpenOrca 7B\", \"Mistral Instruct v0.1 7B\", \"Mistral v0.1 7B\", \"MLewd L2 Chat 13B\", \"MLewdBoros L2 13B\", \"MXLewdMini L2 13B\", \"Mythalion 13B\", \"MythoBoros 13B\", \"MythoLogic L2 13B\", \"MythoLogic Mini 7B\", \"MythoMakiseMerged 13B\", \"MythoMax L2 Kimiko v2 13B\", \"MythoMax L2 13B\", \"MythoMix L2 13B\", \"NewHope 13B\", \"NexusRaven 13B\", \"Nous-Capybara 7B\", \"Nous-Hermes Code 13B\", \"Nous-Hermes L2 13B\", \"Nous-Hermes L2 7B\", \"OpenChat v3.2 13B\", \"OpenOrca Platypus2 13B\", \"Orca Mini v3 13B\", \"Orca Mini v2 7B\", \"PuddleJumper V2 13B\", \"Puma 3B\", \"Pygmalion 2 13B\", \"Pygmalion 2 7B\", \"Pygmalion 2 SuperCOT 13B\", \"Pygmalion 2 SuperCOT Weighed 13B\", \"Redmond Puffin 13B\", \"ReMM SLERP L2 13B\", \"ReMM v2.1 L2 13B\", \"Samantha 1.11 13B\", \"Samantha 7B\", \"Samantha-Mistral 7B\", \"Samantha-Mistral Instruct 7B\", \"Scarlett 13B\", \"Scarlett 7B\", \"Speechless Llama 13B\", \"Speechless Llama2 Hermes Orca Platypus WizardLM 13B\", \"Spicyboros 2.2 13B\", \"Spicyboros 2.2 7B\", \"Spring Dragon 13B\", \"StellarX V0.2 4B\", \"Stheno L2 13B\", \"Stheno Inverted L2 13B\", \"Storytime 13B\", \"Synthia v1.2 13B\", \"Synthia v1.3 7B\", \"TinyLlama Chat v0.3 1.1B\", \"TinyLlama Intermediate Step 480K 1T 1.1B\", \"TinyLlama Python v0.3 1.1B\", \"UltraLM v2.0 13B\", \"UltraRM 13B\", \"UndiMix v2 13B\", \"Unholy v1 10L 13B\", \"Unholy v1 12L 13B\", \"Vicuna v1.5 13B\", \"Vicuna v1.5 7B\", \"WizardCoder Python v1.0 13B\", \"WizardCoder Python v1.0 7B\", \"WizardLM 1.0 L2 13B\", \"WizardLM v1.0 7B\", \"WizardMath v1.0 13B\", \"Wizard-Vicuna 13B\", \"Wizard-Vicuna 7B\", \"Xwin LM V0.1 13B\", \"Xwin LM V0.1 7B\", \"Zarablend L2 7B\"]\n",
        "#@markdown > <font color=\"gray\">GPTQ models from [this list](https://huggingface.co/TheBloke). L2 = Llama 2. B = model size. V = model version.\n",
        "save_to_google_drive = \"off\" #@param [\"off\", \"chatlogs and characters\", \"chatlogs, characters, and models\"]\n",
        "#@markdown > <font color=\"gray\">Remember that chat models are very large, and free Google Drive only provides 15GB.\n",
        "launch_arguments = \"--share --loader exllama --api --public-api --trust-remote-code --chat-buttons\" #@param [\"--share --loader exllama --api --public-api --trust-remote-code\"] {allow-input: true}\n",
        "#@markdown > <font color=\"gray\">You can enter any [command-line flags](https://github.com/oobabooga/text-generation-webui/blob/main/README.md#basic-settings) or [extensions](https://github.com/oobabooga/text-generation-webui/blob/main/docs/Extensions.md) here.\n",
        "debug = False #@param {type:\"boolean\"}\n",
        "#@markdown > <font color=\"gray\">Shows the console log if checked.\n",
        "\n",
        "# Model names\n",
        "match model:\n",
        "\tcase \"Airoboros L2 3.0 13B\": model = \"TheBloke/airoboros-l2-13B-3.0-GPTQ\"\n",
        "\tcase \"Airoboros L2 3.0 7B\": model = \"TheBloke/airoboros-l2-7B-3.0-GPTQ\"\n",
        "\tcase \"Airoboros L2 GPT4 2.0 13B\": model = \"TheBloke/airoboros-l2-13b-gpt4-2.0-GPTQ\"\n",
        "\tcase \"Airoboros L2 GPT4 m2.0 13B\": model = \"TheBloke/airoboros-l2-13b-gpt4-m2.0-GPTQ\"\n",
        "\tcase \"Airoboros L2 GPT4 2.0 7B\": model = \"TheBloke/airoboros-l2-7B-gpt4-2.0-GPTQ\"\n",
        "\tcase \"Airoboros L2 GPT4 m2.0 7B\": model = \"TheBloke/airoboros-l2-7B-gpt4-m2.0-GPTQ\"\n",
        "\tcase \"Airoboros M 3.0 7B\": model = \"TheBloke/airoboros-m-7B-3.0-GPTQ\"\n",
        "\tcase \"Airoboros Mistral2.2 7B\": model = \"TheBloke/airoboros-mistral2.2-7B-GPTQ\"\n",
        "\tcase \"Airochronos L2 13B\": model = \"TheBloke/Airochronos-L2-13B-GPTQ\"\n",
        "\tcase \"Amethyst Mistral 13B\": model = \"TheBloke/Amethyst-13B-Mistral-GPTQ\"\n",
        "\tcase \"AppleSauce L2 13B\": model = \"TheBloke/AppleSauce-L2-13B-GPTQ\"\n",
        "\tcase \"Asclepius 13B\": model = \"TheBloke/Asclepius-13B-GPTQ\"\n",
        "\tcase \"Athena v3 13B\": model = \"TheBloke/Athena-v3-GPTQ\"\n",
        "\tcase \"Baichuan2 Chat 13B\": model = \"TheBloke/Baichuan2-13B-Chat-GPTQ\"\n",
        "\tcase \"BerrySauce L2 13B\": model = \"TheBloke/BerrySauce-L2-13B-GPTQ\"\n",
        "\tcase \"Buddy v0.2 7B\": model = \"TheBloke/Buddy-7B-v0.2-GPTQ\"\n",
        "\tcase \"BlueMethod 13B\": model = \"TheBloke/13B-BlueMethod-GPTQ\"\n",
        "\tcase \"CalliopeDS L2 13B\": model = \"TheBloke/CalliopeDS-L2-13B-GPTQ\"\n",
        "\tcase \"Carl L2 13B\": model = \"TheBloke/Carl-Llama-2-13B-GPTQ\"\n",
        "\tcase \"Chronos-Beluga v2 13B\": model = \"TheBloke/Chronos-Beluga-v2-13B-GPTQ\"\n",
        "\tcase \"Chronos-Hermes v2 13B\": model = \"Austism/chronos-hermes-13b-v2-GPTQ\"\n",
        "\tcase \"CodeFuse 13B\": model = \"TheBloke/CodeFuse-13B-GPTQ\"\n",
        "\tcase \"CodeLlama 13B\": model = \"TheBloke/CodeLlama-13B-GPTQ\"\n",
        "\tcase \"CodeLlama 13B Instruct\": model = \"TheBloke/CodeLlama-13B-Instruct-GPTQ\"\n",
        "\tcase \"CodeLlama 13B Python\": model = \"TheBloke/CodeLlama-13B-Python-GPTQ\"\n",
        "\tcase \"CollectiveCognition v1.1 Mistral 7B\": model = \"TheBloke/CollectiveCognition-v1.1-Mistral-7B-GPTQ\"\n",
        "\tcase \"Dans AdventurousWinds 7B\": model = \"TheBloke/Dans-AdventurousWinds-7B-GPTQ\"\n",
        "\tcase \"Dans TotSirocco 7B\": model = \"TheBloke/Dans-TotSirocco-7B-GPTQ\"\n",
        "\tcase \"Dolphin 2.0 Mistral 7B\": model = \"TheBloke/dolphin-2.0-mistral-7B-GPTQ\"\n",
        "\tcase \"Dolphin L2 7B\": model = \"TheBloke/Dolphin-Llama2-7B-GPTQ\"\n",
        "\tcase \"Emerhyst 13B\": model = \"TheBloke/Emerhyst-13B-GPTQ\"\n",
        "\tcase \"Em German v01 13B\": model = \"TheBloke/em_german_13b_v01-GPTQ\"\n",
        "\tcase \"Em German v01 7B\": model = \"TheBloke/em_german_7b_v01-GPTQ\"\n",
        "\tcase \"Em German Mistral v01 7B\": model = \"TheBloke/em_german_mistral_v01-GPTQ\"\n",
        "\tcase \"Frankenstein's Monster 13B\": model = \"Blackroot/FrankensteinsMonster-13B-GPTQ\"\n",
        "\tcase \"Guanaco L2 13B\": model = \"TheBloke/llama-2-13B-Guanaco-QLoRA-GPTQ\"\n",
        "\tcase \"Guanaco L2 7B\": model = \"TheBloke/llama-2-7B-Guanaco-QLoRA-GPTQ\"\n",
        "\tcase \"Guanaco Uncensored v2 3B\": model = \"TheBloke/Guanaco-3B-Uncensored-v2-GPTQ\"\n",
        "\tcase \"Hermes-Kimiko 13B\": model = \"Blackroot/Hermes-Kimiko-13B-gptq\"\n",
        "\tcase \"Huginn v1.2 13B\": model = \"TheBloke/huginnv1.2-GPTQ\"\n",
        "\tcase \"Huginn v3 13B\": model = \"TheBloke/Huginn-v3-13B-GPTQ\"\n",
        "\tcase \"Huginn v4.5 13B\": model = \"TheBloke/Huginn-13B-v4.5-GPTQ\"\n",
        "\tcase \"JanniesBasedLigma L2 13B\": model = \"TheBloke/JanniesBasedLigma-L2-13B-GPTQ\"\n",
        "\tcase \"Kimiko v2 13B\": model = \"TheBloke/Kimiko-v2-13B-GPTQ\"\n",
        "\tcase \"Kimiko 7B\": model = \"TheBloke/Kimiko-7B-GPTQ\"\n",
        "\tcase \"Koala 7B\": model = \"TheBloke/koala-7B-GPTQ\"\n",
        "\tcase \"Kimiko-Mistral 7B\": model = \"TheBloke/Kimiko-Mistral-7B-GPTQ\"\n",
        "\tcase \"Kuchiki 1.1 L2 7B\": model = \"TheBloke/Kuchiki-1.1-L2-7B-GPTQ\"\n",
        "\tcase \"Leo Hessianai 13B\": model = \"TheBloke/leo-hessianai-13B-GPTQ\"\n",
        "\tcase \"Leo Hessianai 7B\": model = \"TheBloke/leo-hessianai-7B-chat-GPTQ\"\n",
        "\tcase \"Leo Hessianai Chat 13B\": model = \"TheBloke/leo-hessianai-13B-chat-GPTQ\"\n",
        "\tcase \"Leo Hessianai Chat 7B\": model = \"TheBloke/leo-hessianai-7B-GPTQ\"\n",
        "\tcase \"Leo Hessianai Chat Bilingual 13B\": model = \"TheBloke/leo-hessianai-13B-chat-bilingual-GPTQ\"\n",
        "\tcase \"Leo Hessianai Chat Bilingual 7B\": model = \"TheBloke/leo-hessianai-7B-chat-bilingual-GPTQ\"\n",
        "\tcase \"Lince Zero 7B\": model = \"TheBloke/lince-zero-GPTQ\"\n",
        "\tcase \"Llama 2 13B\": model = \"TheBloke/Llama-2-13B-GPTQ\"\n",
        "\tcase \"Llama 2 7B\": model = \"TheBloke/Llama-2-7B-GPTQ\"\n",
        "\tcase \"Llama 2 13B Chat\": model = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
        "\tcase \"Llama 2 7B Chat\": model = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
        "\tcase \"Llama 2 Arguments 7B\": model = \"TheBloke/llama-2-7B-Arguments-GPTQ\"\n",
        "\tcase \"Llama 2 Ensemble v6 13B\": model = \"TheBloke/Llama-2-13B-Ensemble-v6-GPTQ\"\n",
        "\tcase \"Llama 2 Vietnamese 20k 7B\": model = \"TheBloke/Llama-2-7B-vietnamese-20k-GPTQ\"\n",
        "\tcase \"LoKuS 13B\": model = \"TheBloke/LoKuS-13B-GPTQ\"\n",
        "\tcase \"LosslessMegaCoder L2 7B\": model = \"TheBloke/LosslessMegaCoder-Llama2-7B-Mini-GPTQ\"\n",
        "\tcase \"Luban 13B\": model = \"TheBloke/Luban-13B-GPTQ\"\n",
        "\tcase \"Luna AI L2 7B\": model = \"TheBloke/Luna-AI-Llama2-Uncensored-GPTQ\"\n",
        "\tcase \"Magpie 13B\": model = \"TheBloke/Magpie-13B-GPTQ\"\n",
        "\tcase \"MAmmoTH 13B\": model = \"TheBloke/MAmmoTH-13B-GPTQ\"\n",
        "\tcase \"MAmmoTH 7B\": model = \"TheBloke/MAmmoTH-7B-GPTQ\"\n",
        "\tcase \"MAmmoTH Coder 13B\": model = \"TheBloke/MAmmoTH-Coder-13B-GPTQ\"\n",
        "\tcase \"Marx 3B\": model = \"TheBloke/Marx-3b-GPTQ\"\n",
        "\tcase \"MegaMix A1 13B\": model = \"TheBloke/Megamix-A1-13B-GPTQ\"\n",
        "\tcase \"MegaMix S1 13B\": model = \"TheBloke/MegaMix-S1-13B-GPTQ\"\n",
        "\tcase \"MegaMix T1 13B\": model = \"TheBloke/MegaMix-T1-13B-GPTQ\"\n",
        "\tcase \"MetaMath V1.0 13B\": model = \"TheBloke/MetaMath-13B-V1.0-GPTQ\"\n",
        "\tcase \"MetaMath V1.0 7B\": model = \"TheBloke/MetaMath-7B-V1.0-GPTQ\"\n",
        "\tcase \"Metharme 13B\": model = \"TehVenom/Metharme-13b-4bit-GPTQ\"\n",
        "\tcase \"Metharme 7B\": model = \"TehVenom/Metharme-7b-4bit-GPTQ-Safetensors\"\n",
        "\tcase \"Mistral OpenOrca 7B\": model = \"TheBloke/Mistral-7B-OpenOrca-GPTQ\"\n",
        "\tcase \"Mistral Instruct v0.1 7B\": model = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
        "\tcase \"Mistral v0.1 7B\": model = \"TheBloke/Mistral-7B-v0.1-GPTQ\"\n",
        "\tcase \"Mistralic 7B-1\": model = \"TheBloke/Mistralic-7B-1-GPTQ\"\n",
        "\tcase \"MLewd L2 Chat 13B\": model = \"TheBloke/MLewd-L2-Chat-13B-GPTQ\"\n",
        "\tcase \"MLewdBoros L2 13B\": model = \"TheBloke/MLewdBoros-L2-13B-GPTQ\"\n",
        "\tcase \"MXLewdMini L2 13B\": model = \"TheBloke/MXLewdMini-L2-13B-GPTQ\"\n",
        "\tcase \"Mythalion 13B\": model = \"TheBloke/Mythalion-13B-GPTQ\"\n",
        "\tcase \"MythoBoros 13B\": model = \"TheBloke/MythoBoros-13B-GPTQ\"\n",
        "\tcase \"MythoLogic L2 13B\": model = \"TheBloke/MythoLogic-L2-13B-GPTQ\"\n",
        "\tcase \"MythoLogic Mini 7B\": model = \"TheBloke/MythoLogic-Mini-7B-GPTQ\"\n",
        "\tcase \"MythoMakiseMerged 13B\": model = \"TheBloke/MythoMakiseMerged-13B-GPTQ\"\n",
        "\tcase \"MythoMax L2 Kimiko v2 13B\": model = \"TheBloke/MythoMax-L2-Kimiko-v2-13B-GPTQ\"\n",
        "\tcase \"MythoMax L2 13B\": model = \"TheBloke/MythoMax-L2-13B-GPTQ\"\n",
        "\tcase \"MythoMix L2 13B\": model = \"TheBloke/MythoMix-L2-13B-GPTQ\"\n",
        "\tcase \"NewHope 13B\": model = \"TheBloke/NewHope-GPTQ\"\n",
        "\tcase \"NexusRaven 13B\": model = \"TheBloke/NexusRaven-13B-GPTQ\"\n",
        "\tcase \"Nous-Capybara 7B\": model = \"TheBloke/Nous-Capybara-7B-GPTQ\"\n",
        "\tcase \"Nous-Hermes Code 13B\": model = \"TheBloke/Nous-Hermes-13B-Code-GPTQ\"\n",
        "\tcase \"Nous-Hermes L2 13B\": model = \"TheBloke/Nous-Hermes-Llama2-GPTQ\"\n",
        "\tcase \"Nous-Hermes L2 7B\": model = \"TheBloke/Nous-Hermes-Llama-2-7B-GPTQ\"\n",
        "\tcase \"OpenChat v3.2 13B\": model = \"TheBloke/OpenChat_v3.2-GPTQ\"\n",
        "\tcase \"OpenOrca Platypus2 13B\": model = \"TheBloke/OpenOrca-Platypus2-13B-GPTQ\"\n",
        "\tcase \"Orca Mini v3 13B\": model = \"TheBloke/orca_mini_v3_13B-GPTQ\"\n",
        "\tcase \"Orca Mini v2 7B\": model = \"TheBloke/orca_mini_v2_7B-GPTQ\"\n",
        "\tcase \"PuddleJumper V2 13B\": model = \"TheBloke/PuddleJumper-13B-V2-GPTQ\"\n",
        "\tcase \"Puma 3B\": model = \"TheBloke/Puma-3b-GPTQ\"\n",
        "\tcase \"Pygmalion 2 13B\": model = \"TheBloke/Pygmalion-2-13B-GPTQ\"\n",
        "\tcase \"Pygmalion 2 7B\": model = \"TheBloke/Pygmalion-2-7B-GPTQ\"\n",
        "\tcase \"Pygmalion 2 SuperCOT 13B\": model = \"TheBloke/Pygmalion-2-13B-SuperCOT-GPTQ\"\n",
        "\tcase \"Pygmalion 2 SuperCOT Weighed 13B\": model = \"TheBloke/Pygmalion-2-13B-SuperCOT-weighed-GPTQ\"\n",
        "\tcase \"Redmond Puffin 13B\": model = \"TheBloke/Redmond-Puffin-13B-GPTQ\"\n",
        "\tcase \"ReMM SLERP L2 13B\": model = \"TheBloke/ReMM-SLERP-L2-13B-GPTQ\"\n",
        "\tcase \"ReMM v2.1 L2 13B\": model = \"TheBloke/ReMM-v2.1-L2-13B-GPTQ\"\n",
        "\tcase \"Samantha 1.11 13B\": model = \"TheBloke/Samantha-1.11-13B-GPTQ\"\n",
        "\tcase \"Samantha 7B\": model = \"TheBloke/Samantha-7B-GPTQ\"\n",
        "\tcase \"Samantha-Mistral 7B\": model = \"TheBloke/samantha-mistral-7B-GPTQ\"\n",
        "\tcase \"Samantha-Mistral Instruct 7B\": model = \"TheBloke/samantha-mistral-instruct-7B-GPTQ\"\n",
        "\tcase \"Scarlett 13B\": model = \"TheBloke/Scarlett-13B-GPTQ\"\n",
        "\tcase \"Scarlett 7B\": model = \"TheBloke/Scarlett-7B-GPTQ\"\n",
        "\tcase \"Speechless Llama 13B\": model = \"TheBloke/Speechless-Llama2-13B-GPTQ\"\n",
        "\tcase \"Speechless Llama2 Hermes Orca Platypus WizardLM 13B\": model = \"TheBloke/Speechless-Llama2-Hermes-Orca-Platypus-WizardLM-13B-GPTQ\"\n",
        "\tcase \"Spicyboros 2.2 13B\": model = \"TheBloke/Spicyboros-13B-2.2-GPTQ\"\n",
        "\tcase \"Spicyboros 2.2 7B\": model = \"TheBloke/Spicyboros-7B-2.2-GPTQ\"\n",
        "\tcase \"Spring Dragon 13B\": model = \"TheBloke/Spring-Dragon-GPTQ\"\n",
        "\tcase \"StellarX V0.2 4B\": model = \"TheBloke/StellarX-4B-V0.2-GPTQ\"\n",
        "\tcase \"Stheno L2 13B\": model = \"TheBloke/Stheno-L2-13B-GPTQ\"\n",
        "\tcase \"Stheno Inverted L2 13B\": model = \"TheBloke/Stheno-Inverted-L2-13B-GPTQ\"\n",
        "\tcase \"Storytime 13B\": model = \"TheBloke/storytime-13B-GPTQ\"\n",
        "\tcase \"Synthia v1.2 13B\": model = \"TheBloke/Synthia-13B-v1.2-GPTQ\"\n",
        "\tcase \"Synthia v1.3 7B\": model = \"TheBloke/Synthia-7B-v1.3-GPTQ\"\n",
        "\tcase \"TinyLlama Chat v0.3 1.1B\": model = \"TheBloke/TinyLlama-1.1B-Chat-v0.3-GPTQ\"\n",
        "\tcase \"TinyLlama Intermediate Step 480K 1T 1.1B\": model = \"TheBloke/TinyLlama-1.1B-intermediate-step-480k-1T-GPTQ\"\n",
        "\tcase \"TinyLlama Python v0.3 1.1B\": model = \"TheBloke/TinyLlama-1.1B-python-v0.1-GPTQ\"\n",
        "\tcase \"UltraLM v2.0 13B\": model = \"TheBloke/UltraLM-13B-v2.0-GPTQ\"\n",
        "\tcase \"UltraRM 13B\": model = \"TheBloke/UltraRM-13B-GPTQ\"\n",
        "\tcase \"UndiMix v2 13B\": model = \"TheBloke/UndiMix-v2-13B-GPTQ\"\n",
        "\tcase \"Unholy v1 10L 13B\": model = \"TheBloke/Unholy-v1-10l-13B-GPTQ\"\n",
        "\tcase \"Unholy v1 12L 13B\": model = \"TheBloke/Unholy-v1-12L-13B-GPTQ\"\n",
        "\tcase \"Vicuna v1.5 13B\": model = \"TheBloke/vicuna-13B-v1.5-GPTQ\"\n",
        "\tcase \"Vicuna v1.5 7B\": model = \"TheBloke/vicuna-7B-v1.5-GPTQ\"\n",
        "\tcase \"WizardCoder Python v1.0 13B\": model = \"TheBloke/WizardCoder-Python-13B-V1.0-GPTQ\"\n",
        "\tcase \"WizardCoder Python v1.0 7B\": model = \"TheBloke/WizardCoder-Python-7B-V1.0-GPTQ\"\n",
        "\tcase \"WizardLM 1.0 L2 13B\": model = \"TheBloke/WizardLM-1.0-Uncensored-Llama2-13B-GPTQ\"\n",
        "\tcase \"WizardLM v1.0 7B\": model = \"TheBloke/WizardLM-7B-V1.0-Uncensored-GPTQ\"\n",
        "\tcase \"WizardMath v1.0 13B\": model = \"TheBloke/WizardMath-13B-V1.0-GPTQ\"\n",
        "\tcase \"Wizard-Vicuna 13B\": model = \"TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ\"\n",
        "\tcase \"Wizard-Vicuna 7B\": model = \"TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ\"\n",
        "\tcase \"Xwin LM V0.1 13B\": model = \"TheBloke/Xwin-LM-13B-V0.1-GPTQ\"\n",
        "\tcase \"Xwin LM V0.1 7B\": model = \"TheBloke/Xwin-LM-7B-V0.1-GPTQ\"\n",
        "\tcase \"Zarablend L2 7B\": model = \"TheBloke/Zarablend-L2-7B-GPTQ\"\n",
        "\n",
        "# Install ooba\n",
        "def install_ooba():\n",
        "\tif os.path.exists(repo_dir):\n",
        "\t\t%cd {repo_dir}\n",
        "\t\t!git pull\n",
        "\telse:\n",
        "\t\t!git clone https://github.com/jimdb8687/text-generation-webui.git\n",
        "\tif (\"chatlogs and characters\" in save_to_google_drive):\n",
        "\t\tif not os.path.exists(f\"{base_drive_dir}/oobabooga-data\"):\n",
        "\t\t\tos.mkdir(f\"{base_drive_dir}/oobabooga-data\")\n",
        "\t\tif not os.path.exists(f\"{base_drive_dir}/oobabooga-data/logs\"):\n",
        "\t\t\tos.mkdir(f\"{base_drive_dir}/oobabooga-data/logs\")\n",
        "\t\tif not os.path.exists(f\"{base_drive_dir}/oobabooga-data/characters\"):\n",
        "\t\t\tshutil.move(\"text-generation-webui/characters\", f\"{base_drive_dir}/oobabooga-data/characters\")\n",
        "\t\telse:\n",
        "\t\t\t!rm -r \"text-generation-webui/characters\"\n",
        "\n",
        "\t\t!ln -s \"$base_drive_dir/oobabooga-data/logs\" \"text-generation-webui/logs\"\n",
        "\t\t!ln -s \"$base_drive_dir/oobabooga-data/characters\" \"text-generation-webui/characters\"\n",
        "\telse:\n",
        "\t\t!mkdir text-generation-webui/logs\n",
        "\t!ln -s text-generation-webui/logs .\n",
        "\t!ln -s text-generation-webui/characters .\n",
        "\t!ln -s text-generation-webui/models .\n",
        "\t%rm -r sample_data\n",
        "\t%cd {repo_dir}\n",
        "\t!pip install -r requirements.txt\n",
        "\n",
        "# Mount Google Drive\n",
        "if not (\"off\" in save_to_google_drive):\n",
        "\tif (\"chatlogs and characters\" in save_to_google_drive):\n",
        "\t\tdrive.mount('/content/drive')\n",
        "\t\tbase_drive_dir = \"/content/drive/MyDrive/\"\n",
        "\t\trepo_dir = '/content/text-generation-webui'\n",
        "\t\t%cd /content\n",
        "\t\tif (debug is False):\n",
        "\t\t\tclear_output(wait = True)\n",
        "\t\t\tprint(f\"\\033[1;32;1m\\n######################################################\\n\\nBeginning installation. This will take a few minutes.\\n\\nInstalling WebUI...\\n\\n######################################################\\n\\033[0;37;0m\")\n",
        "\t\t\twith capture.capture_output() as cap:\n",
        "\t\t\t\tinstall_ooba()\n",
        "\t\telse:\n",
        "\t\t\tinstall_ooba()\n",
        "\tif (\"chatlogs, characters, and models\" in save_to_google_drive):\n",
        "\t\tdrive.mount('/content/drive')\n",
        "\t\tbase_drive_dir = \"/content/drive/MyDrive/\"\n",
        "\t\trepo_dir = '/content/drive/MyDrive/text-generation-webui'\n",
        "\t\tmodel_dir = '/content/drive/MyDrive/text-generation-webui/models'\n",
        "\t\t%cd /content/drive/MyDrive\n",
        "\t\tif (debug is False):\n",
        "\t\t\tclear_output(wait = True)\n",
        "\t\t\tprint(f\"\\033[1;32;1m\\n######################################################\\n\\nBeginning installation. This will take a few minutes.\\n\\nInstalling WebUI...\\n\\n######################################################\\n\\033[0;37;0m\")\n",
        "\t\t\twith capture.capture_output() as cap:\n",
        "\t\t\t\tinstall_ooba()\n",
        "\t\telse:\n",
        "\t\t\tinstall_ooba()\n",
        "else:\n",
        "\t%cd /content\n",
        "\trepo_dir = '/content/text-generation-webui'\n",
        "\tmodel_dir = '/content/text-generation-webui/models'\n",
        "\tif (debug is False):\n",
        "\t\tclear_output(wait = True)\n",
        "\t\tprint(f\"\\033[1;32;1m\\n######################################################\\n\\nBeginning installation. This will take a few minutes.\\n\\nInstalling WebUI...\\n\\n######################################################\\n\\033[0;37;0m\")\n",
        "\t\twith capture.capture_output() as cap:\n",
        "\t\t\tinstall_ooba()\n",
        "\telse:\n",
        "\t\tinstall_ooba()\n",
        "\n",
        "# Model download\n",
        "%cd {repo_dir}\n",
        "if (debug is False):\n",
        "\tclear_output(wait = True)\n",
        "\tprint(f\"\\033[1;32;1m\\n######################################################\\n\\nBeginning installation. This will take a few minutes.\\n\\nWebUI installed. Downloading model...\\n\\n######################################################\\n\\033[0;37;0m\")\n",
        "\twith capture.capture_output() as cap:\n",
        "\t\t!python download-model.py {model}\n",
        "else:\n",
        "\t!python download-model.py {model}\n",
        "model = model.replace('/', '_')\n",
        "\n",
        "\n",
        "# Install extension requirements\n",
        "if ('deepspeed' in launch_arguments):\n",
        "\tif (debug is False):\n",
        "\t\twith capture.capture_output() as cap:\n",
        "\t\t\t!pip install -U mpi4py\n",
        "\t\t\t!pip install -U deepspeed\n",
        "\telse:\n",
        "\t\t!pip install -U mpi4py\n",
        "\t\t!pip install -U deepspeed\n",
        "if ('xformers' in launch_arguments):\n",
        "\tif (debug is False):\n",
        "\t\twith capture.capture_output() as cap:\n",
        "\t\t\t!pip install xformers\n",
        "\telse:\n",
        "\t\t!pip install xformers\n",
        "if ('api' in launch_arguments):\n",
        "\tif (debug is False):\n",
        "\t\twith capture.capture_output() as cap:\n",
        "\t\t\t!pip install -r extensions/api/requirements.txt\n",
        "\telse:\n",
        "\t\t!pip install -r extensions/api/requirements.txt\n",
        "if ('google_translate' in launch_arguments):\n",
        "\tif (debug is False):\n",
        "\t\twith capture.capture_output() as cap:\n",
        "\t\t\t!pip install -r extensions/google_translate/requirements.txt\n",
        "\telse:\n",
        "\t\t!pip install -r extensions/google_translate/requirements.txt\n",
        "if ('superbooga' in launch_arguments):\n",
        "\tif (debug is False):\n",
        "\t\twith capture.capture_output() as cap:\n",
        "\t\t\t!pip install -r extensions/superbooga/requirements.txt\n",
        "\telse:\n",
        "\t\t!pip install -r extensions/superbooga/requirements.txt\n",
        "if ('silero_tts' in launch_arguments):\n",
        "\tif (debug is False):\n",
        "\t\twith capture.capture_output() as cap:\n",
        "\t\t\t!pip install -r extensions/silero_tts/requirements.txt\n",
        "\telse:\n",
        "\t\t!pip install -r extensions/silero_tts/requirements.txt\n",
        "if ('elevenlabs_tts' in launch_arguments):\n",
        "\tif (debug is False):\n",
        "\t\twith capture.capture_output() as cap:\n",
        "\t\t\t!pip install -r extensions/elevenlabs_tts/requirements.txt\n",
        "\telse:\n",
        "\t\t!pip install -r extensions/silero_tts/requirements.txt\n",
        "if ('whisper_stt' in launch_arguments):\n",
        "\tif (debug is False):\n",
        "\t\twith capture.capture_output() as cap:\n",
        "\t\t\t!pip install -r extensions/whisper_stt/requirements.txt\n",
        "\telse:\n",
        "\t\t!pip install -r extensions/silero_tts/requirements.txt\n",
        "if ('openai' in launch_arguments):\n",
        "\tif (debug is False):\n",
        "\t\twith capture.capture_output() as cap:\n",
        "\t\t\t!pip install -r extensions/openai/requirements.txt\n",
        "\telse:\n",
        "\t\t!pip install -r extensions/silero_tts/requirements.txt\n",
        "if ('ngrok' in launch_arguments):\n",
        "\tif (debug is False):\n",
        "\t\twith capture.capture_output() as cap:\n",
        "\t\t\t!pip install -r extensions/ngrok/requirements.txt\n",
        "\telse:\n",
        "\t\t!pip install -r extensions/silero_tts/requirements.txt\n",
        "\n",
        "# Run WebUI\n",
        "if (debug is False):\n",
        "\tclear_output(wait = True)\n",
        "\tprint(f\"\\033[1;32;1m\\n######################################################\\n\\nInstallation complete. The model is now loading. To enter the WebUI, click on the gradio.live link that will appear below in about a minute.\\n\\nFor SillyTavern users, copy the non-streaming URL (ends with \\\"/api\\\") and paste it into the \\\"Blocking API URL\\\" in SillyTavern's API settings.\\n\\n######################################################\\n\\033[0;37;0m\")\n",
        "if ('deepspeed' in launch_arguments):\n",
        "  cmd =f\"deepspeed --num_gpus=1 server.py --model {model} {launch_arguments}\"\n",
        "  print(cmd)\n",
        "  !$cmd\n",
        "else:\n",
        "  cmd = f\"python server.py --model {model} {launch_arguments}\"\n",
        "  print(cmd)\n",
        "  !$cmd"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}